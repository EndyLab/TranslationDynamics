{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cognate ternary complex distribution for a given tRNA and codon distribution (i.e., at a given growth rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from analysis_utils import *\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "import pandas as pd\n",
    "ptRNA = pd.read_excel('tRNAValues.xlsx',header=None)\n",
    "gr04_ptRNA=ptRNA[0]\n",
    "gr07_ptRNA=ptRNA[1]\n",
    "gr107_ptRNA=ptRNA[2]\n",
    "gr16_ptRNA=ptRNA[3]\n",
    "gr25_ptRNA=ptRNA[4]\n",
    "\n",
    "pCodon = pd.read_excel('codonValues.xlsx',header=None)\n",
    "gr04_pCodon = pCodon[0]\n",
    "gr07_pCodon = pCodon[1]\n",
    "gr107_pCodon = pCodon[2]\n",
    "gr16_pCodon = pCodon[3]\n",
    "gr25_pCodon = pCodon[4]\n",
    "\n",
    "rxndiff=dict()\n",
    "\n",
    "#p_codon_count_hist_weighted_avg=cognateDistrib(ptRNA,pCodon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "missing expt\n",
      "545\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "missing expt\n",
      "804\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Unweighted search time ( 1  cognate) 426.68922867046115\n",
      "Unweighted search time ( 2  cognate) 177.497896933155\n",
      "Unweighted search time ( 3  cognate) 120.30314389849794\n",
      "Unweighted search time ( 4  cognate) 95.84486115037801\n",
      "Unweighted search time ( 5  cognate) 90.00181629104335\n",
      "Unweighted search time ( 6  cognate) 68.66961539258227\n",
      "Unweighted search time ( 7  cognate) 54.653481569849184\n",
      "Unweighted search time ( 8  cognate) 52.0401348919299\n",
      "Unweighted search time ( 9  cognate) 31.30225674888329\n",
      "Unweighted search time ( 10  cognate) 36.90531355414013\n",
      "Unweighted search time ( 11  cognate) 29.336951952329017\n",
      "Unweighted search time ( 12  cognate) nan\n",
      "Unweighted search time ( 13  cognate) nan\n",
      "Unweighted search time ( 14  cognate) nan\n",
      "Unweighted search time ( 15  cognate) nan\n",
      "Unweighted search time ( 16  cognate) nan\n",
      "Unweighted search time ( 17  cognate) nan\n",
      "Unweighted search time ( 18  cognate) nan\n",
      "Unweighted search time ( 19  cognate) nan\n",
      "Unweighted search time ( 20  cognate) nan\n",
      "Unweighted search time ( 21  cognate) nan\n",
      "Unweighted search time ( 22  cognate) nan\n",
      "Unweighted search time ( 23  cognate) nan\n",
      "Unweighted search time ( 24  cognate) nan\n",
      "Unweighted search time ( 25  cognate) nan\n",
      "Unweighted search time ( 26  cognate) nan\n",
      "Unweighted search time ( 27  cognate) nan\n",
      "Unweighted search time ( 28  cognate) nan\n",
      "Unweighted search time ( 29  cognate) nan\n",
      "Unweighted search time ( 30  cognate) nan\n",
      "Unweighted search time ( 31  cognate) nan\n",
      "Unweighted search time ( 32  cognate) nan\n",
      "Unweighted search time ( 33  cognate) nan\n",
      "Transport time:  [159.87371455298333]  +/-  [9.901508225843417]\n",
      "Reaction time:  [13.212476490396053]  +/-  [1.1485588023363071]\n",
      "Search time:  [173.0861910433794]  +/-  [10.695745882807273]\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "missing expt\n",
      "218\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Unweighted search time ( 1  cognate) 389.29162033299633\n",
      "Unweighted search time ( 2  cognate) 195.2640953085154\n",
      "Unweighted search time ( 3  cognate) 115.34566994307104\n",
      "Unweighted search time ( 4  cognate) 99.51536542228067\n",
      "Unweighted search time ( 5  cognate) 68.9457861981281\n",
      "Unweighted search time ( 6  cognate) 51.8572232535522\n",
      "Unweighted search time ( 7  cognate) 42.36324950612762\n",
      "Unweighted search time ( 8  cognate) 34.93646336215733\n",
      "Unweighted search time ( 9  cognate) 31.488883741736444\n",
      "Unweighted search time ( 10  cognate) 33.00739226279752\n",
      "Unweighted search time ( 11  cognate) 24.414766481132336\n",
      "Unweighted search time ( 12  cognate) nan\n",
      "Unweighted search time ( 13  cognate) nan\n",
      "Unweighted search time ( 14  cognate) nan\n",
      "Unweighted search time ( 15  cognate) nan\n",
      "Unweighted search time ( 16  cognate) nan\n",
      "Unweighted search time ( 17  cognate) nan\n",
      "Unweighted search time ( 18  cognate) nan\n",
      "Unweighted search time ( 19  cognate) nan\n",
      "Unweighted search time ( 20  cognate) nan\n",
      "Unweighted search time ( 21  cognate) nan\n",
      "Unweighted search time ( 22  cognate) nan\n",
      "Unweighted search time ( 23  cognate) nan\n",
      "Unweighted search time ( 24  cognate) nan\n",
      "Unweighted search time ( 25  cognate) nan\n",
      "Unweighted search time ( 26  cognate) nan\n",
      "Unweighted search time ( 27  cognate) nan\n",
      "Unweighted search time ( 28  cognate) nan\n",
      "Unweighted search time ( 29  cognate) nan\n",
      "Unweighted search time ( 30  cognate) nan\n",
      "Unweighted search time ( 31  cognate) nan\n",
      "Unweighted search time ( 32  cognate) nan\n",
      "Unweighted search time ( 33  cognate) nan\n",
      "Transport time:  [138.57305205502985]  +/-  [10.653644361128856]\n",
      "Reaction time:  [27.95698625157086]  +/-  [2.363939317592775]\n",
      "Search time:  [166.53003830660072]  +/-  [12.813555267286414]\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "missing expt\n",
      "830\n",
      "Computing...\n",
      "Computing...\n",
      "missing expt\n",
      "116\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "missing expt\n",
      "728\n",
      "missing expt\n",
      "792\n",
      "Computing...\n",
      "Unweighted search time ( 1  cognate) 271.474693309938\n",
      "Unweighted search time ( 2  cognate) 145.60632549023543\n",
      "Unweighted search time ( 3  cognate) 106.52609173286244\n",
      "Unweighted search time ( 4  cognate) 53.94149452496741\n",
      "Unweighted search time ( 5  cognate) 56.314901899644745\n",
      "Unweighted search time ( 6  cognate) 42.99186461828626\n",
      "Unweighted search time ( 7  cognate) 43.046872878365576\n",
      "Unweighted search time ( 8  cognate) 26.504863286084472\n",
      "Unweighted search time ( 9  cognate) 26.81757961619516\n",
      "Unweighted search time ( 10  cognate) 26.595778612210786\n",
      "Unweighted search time ( 11  cognate) 22.149406681818288\n",
      "Unweighted search time ( 12  cognate) nan\n",
      "Unweighted search time ( 13  cognate) nan\n",
      "Unweighted search time ( 14  cognate) nan\n",
      "Unweighted search time ( 15  cognate) nan\n",
      "Unweighted search time ( 16  cognate) nan\n",
      "Unweighted search time ( 17  cognate) nan\n",
      "Unweighted search time ( 18  cognate) nan\n",
      "Unweighted search time ( 19  cognate) nan\n",
      "Unweighted search time ( 20  cognate) nan\n",
      "Unweighted search time ( 21  cognate) nan\n",
      "Unweighted search time ( 22  cognate) nan\n",
      "Unweighted search time ( 23  cognate) nan\n",
      "Unweighted search time ( 24  cognate) nan\n",
      "Unweighted search time ( 25  cognate) nan\n",
      "Unweighted search time ( 26  cognate) nan\n",
      "Unweighted search time ( 27  cognate) nan\n",
      "Unweighted search time ( 28  cognate) nan\n",
      "Unweighted search time ( 29  cognate) nan\n",
      "Unweighted search time ( 30  cognate) nan\n",
      "Unweighted search time ( 31  cognate) nan\n",
      "Unweighted search time ( 32  cognate) nan\n",
      "Unweighted search time ( 33  cognate) nan\n",
      "Transport time:  [95.10248766030087]  +/-  [6.474264823846152]\n",
      "Reaction time:  [22.601519734409734]  +/-  [1.7024913106868653]\n",
      "Search time:  [117.7040073947106]  +/-  [7.998331363382792]\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "missing expt\n",
      "327\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Unweighted search time ( 1  cognate) 200.2249530232702\n",
      "Unweighted search time ( 2  cognate) 108.36198572461855\n",
      "Unweighted search time ( 3  cognate) 64.79356745270022\n",
      "Unweighted search time ( 4  cognate) 56.21415298033055\n",
      "Unweighted search time ( 5  cognate) 35.70904091917134\n",
      "Unweighted search time ( 6  cognate) 32.117417490905076\n",
      "Unweighted search time ( 7  cognate) 27.07666228982147\n",
      "Unweighted search time ( 8  cognate) 26.64127795783609\n",
      "Unweighted search time ( 9  cognate) 17.740805025167063\n",
      "Unweighted search time ( 10  cognate) 13.493403162365624\n",
      "Unweighted search time ( 11  cognate) 14.792437974779634\n",
      "Unweighted search time ( 12  cognate) nan\n",
      "Unweighted search time ( 13  cognate) nan\n",
      "Unweighted search time ( 14  cognate) nan\n",
      "Unweighted search time ( 15  cognate) nan\n",
      "Unweighted search time ( 16  cognate) nan\n",
      "Unweighted search time ( 17  cognate) nan\n",
      "Unweighted search time ( 18  cognate) nan\n",
      "Unweighted search time ( 19  cognate) nan\n",
      "Unweighted search time ( 20  cognate) nan\n",
      "Unweighted search time ( 21  cognate) nan\n",
      "Unweighted search time ( 22  cognate) nan\n",
      "Unweighted search time ( 23  cognate) nan\n",
      "Unweighted search time ( 24  cognate) nan\n",
      "Unweighted search time ( 25  cognate) nan\n",
      "Unweighted search time ( 26  cognate) nan\n",
      "Unweighted search time ( 27  cognate) nan\n",
      "Unweighted search time ( 28  cognate) nan\n",
      "Unweighted search time ( 29  cognate) nan\n",
      "Unweighted search time ( 30  cognate) nan\n",
      "Unweighted search time ( 31  cognate) nan\n",
      "Unweighted search time ( 32  cognate) nan\n",
      "Unweighted search time ( 33  cognate) nan\n",
      "Transport time:  [69.43417650803316]  +/-  [4.299175755047592]\n",
      "Reaction time:  [16.82543570612495]  +/-  [1.2256281598119028]\n",
      "Search time:  [86.25961221415812]  +/-  [5.427296447487777]\n",
      "Computing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing...\n",
      "Computing...\n",
      "missing expt\n",
      "202\n",
      "Computing...\n",
      "Computing...\n",
      "missing expt\n",
      "462\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "missing expt\n",
      "510\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "missing expt\n",
      "447\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Unweighted search time ( 1  cognate) 200.1169937894533\n",
      "Unweighted search time ( 2  cognate) 84.02792722842412\n",
      "Unweighted search time ( 3  cognate) 61.07896381228008\n",
      "Unweighted search time ( 4  cognate) 40.48318424533152\n",
      "Unweighted search time ( 5  cognate) 40.73540242572679\n",
      "Unweighted search time ( 6  cognate) 26.343668227923853\n",
      "Unweighted search time ( 7  cognate) 23.692638573720732\n",
      "Unweighted search time ( 8  cognate) 20.72395640321115\n",
      "Unweighted search time ( 9  cognate) 16.560329391048032\n",
      "Unweighted search time ( 10  cognate) 13.230331478053982\n",
      "Unweighted search time ( 11  cognate) 14.175397461077793\n",
      "Unweighted search time ( 12  cognate) nan\n",
      "Unweighted search time ( 13  cognate) nan\n",
      "Unweighted search time ( 14  cognate) nan\n",
      "Unweighted search time ( 15  cognate) nan\n",
      "Unweighted search time ( 16  cognate) nan\n",
      "Unweighted search time ( 17  cognate) nan\n",
      "Unweighted search time ( 18  cognate) nan\n",
      "Unweighted search time ( 19  cognate) nan\n",
      "Unweighted search time ( 20  cognate) nan\n",
      "Unweighted search time ( 21  cognate) nan\n",
      "Unweighted search time ( 22  cognate) nan\n",
      "Unweighted search time ( 23  cognate) nan\n",
      "Unweighted search time ( 24  cognate) nan\n",
      "Unweighted search time ( 25  cognate) nan\n",
      "Unweighted search time ( 26  cognate) nan\n",
      "Unweighted search time ( 27  cognate) nan\n",
      "Unweighted search time ( 28  cognate) nan\n",
      "Unweighted search time ( 29  cognate) nan\n",
      "Unweighted search time ( 30  cognate) nan\n",
      "Unweighted search time ( 31  cognate) nan\n",
      "Unweighted search time ( 32  cognate) nan\n",
      "Unweighted search time ( 33  cognate) nan\n",
      "Transport time:  [66.09090236553472]  +/-  [4.008698965169225]\n",
      "Reaction time:  [13.251320690462379]  +/-  [0.8812627966445516]\n",
      "Search time:  [79.34222305599708]  +/-  [4.801175758450715]\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n",
      "Computing...\n"
     ]
    }
   ],
   "source": [
    "#Growth rate = 0.6\n",
    "from analysis_utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "path = \"/Users/Akshay/Documents/TranslationDynamics/data/\"\n",
    "\n",
    "gr_sim = dict()\n",
    "\n",
    "data = \"191003_2140/\"\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "data = \"191105_1738/\"\n",
    "for i in range(7,16):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-6),100*(i-1-6)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "data = \"191105_1817/\"\n",
    "for i in range(16,25):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-15),100*(i-1-15)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "    \n",
    "data = \"191105_1934/\"\n",
    "for i in range(25,34):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-24),100*(i-1-24)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "\n",
    "transportRxnResults = transportRxnCalc(gr_dict,gr07_ptRNA,gr07_pCodon)\n",
    "\n",
    "search_list = transportRxnResults[0]\n",
    "rxndiff['06'] = transportRxnResults[1:]\n",
    "\n",
    "\n",
    "#Growth rate = 1.0\n",
    "from analysis_utils import *\n",
    "path = \"/Users/Akshay/Documents/TranslationDynamics/data/\"\n",
    "\n",
    "gr_sim = dict()\n",
    "\n",
    "data = \"191004_0018/\"\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "data = \"191105_2012/\"\n",
    "for i in range(7,16):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-6),100*(i-1-6)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "data = \"191105_2115/\"\n",
    "for i in range(16,25):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-15),100*(i-1-15)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "    \n",
    "data = \"191105_2238/\"\n",
    "for i in range(25,34):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-24),100*(i-1-24)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "\n",
    "transportRxnResults = transportRxnCalc(gr_dict,gr107_ptRNA,gr107_pCodon)\n",
    "\n",
    "search_list = transportRxnResults[0]\n",
    "rxndiff['10'] = transportRxnResults[1:]\n",
    "\n",
    "#Growth rate = 1.5\n",
    "from analysis_utils import *\n",
    "path = \"/Users/Akshay/Documents/TranslationDynamics/data/\"\n",
    "\n",
    "gr_sim = dict()\n",
    "\n",
    "data = \"191004_0104/\"\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "data = \"191106_0916/\"\n",
    "for i in range(7,16):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-6),100*(i-1-6)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "data = \"191106_1041/\"\n",
    "for i in range(16,25):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-15),100*(i-1-15)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "    \n",
    "data = \"191106_1206/\"\n",
    "for i in range(25,34):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-24),100*(i-1-24)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "\n",
    "transportRxnResults = transportRxnCalc(gr_dict,gr16_ptRNA,gr16_pCodon)\n",
    "\n",
    "search_list = transportRxnResults[0]\n",
    "rxndiff['15'] = transportRxnResults[1:]\n",
    "\n",
    "#Growth rate = 2.0\n",
    "from analysis_utils import *\n",
    "path = \"/Users/Akshay/Documents/TranslationDynamics/data/\"\n",
    "\n",
    "gr_sim = dict()\n",
    "\n",
    "data = \"191004_1129/\"\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "data = \"191106_1348/\"\n",
    "for i in range(7,16):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-6),100*(i-1-6)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "data = \"191106_1428/\"\n",
    "for i in range(16,25):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-15),100*(i-1-15)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "    \n",
    "data = \"191106_1506/\"\n",
    "for i in range(25,34):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-24),100*(i-1-24)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "\n",
    "transportRxnResults = transportRxnCalc(gr_dict,gr16_ptRNA,gr16_pCodon)\n",
    "\n",
    "search_list = transportRxnResults[0]\n",
    "rxndiff['20'] = transportRxnResults[1:]\n",
    "\n",
    "#Growth rate = 2.5\n",
    "from analysis_utils import *\n",
    "path = \"/Users/Akshay/Documents/TranslationDynamics/data/\"\n",
    "\n",
    "gr_sim = dict()\n",
    "\n",
    "data = \"191004_1217/\"\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "data = \"191106_1556/\"\n",
    "for i in range(7,16):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-6),100*(i-1-6)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "data = \"191106_1714/\"\n",
    "for i in range(16,25):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-15),100*(i-1-15)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "    \n",
    "data = \"191106_1757/\"\n",
    "for i in range(25,34):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-24),100*(i-1-24)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "\n",
    "transportRxnResults = transportRxnCalc(gr_dict,gr25_ptRNA,gr25_pCodon)\n",
    "\n",
    "search_list = transportRxnResults[0]\n",
    "rxndiff['25'] = transportRxnResults[1:]\n",
    "\n",
    "#Growth rate = 3.0\n",
    "from analysis_utils import *\n",
    "path = \"/Users/Akshay/Documents/TranslationDynamics/data/\"\n",
    "\n",
    "gr_sim = dict()\n",
    "\n",
    "data = \"191003_2210/\"\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "data = \"191105_0908/\"\n",
    "for i in range(7,16):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-6),100*(i-1-6)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "data = \"191105_0951/\"\n",
    "for i in range(16,25):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-15),100*(i-1-15)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "    \n",
    "data = \"191105_1033/\"\n",
    "for i in range(25,34):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1-24),100*(i-1-24)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "\n",
    "\n",
    "transportRxnResults = transportRxnCalc(gr_dict,gr25_ptRNA,gr25_pCodon)\n",
    "\n",
    "search_list = transportRxnResults[0]\n",
    "rxndiff['30'] = transportRxnResults[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plotting Figure 6 ###\n",
    "from analysis_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib as mpl\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "hfont = {'fontname':'Helvetica'}\n",
    "\n",
    "colors = ['darkblue','#D43F3A']\n",
    "gr_i_list = [0.6,1.0,1.5,2.0,2.5,3.0]\n",
    "phi_list = np.array([0.13172292, 0.22350473, 0.30181037, 0.35583449, 0.3945994 ,\n",
    "       0.42317604])\n",
    "markers = ['*','^']\n",
    "\n",
    "fig,[[ax1,ax2],[ax3,ax4]]= plt.subplots(2,2,figsize=(16,16))\n",
    "\n",
    "\n",
    "###Plot Elongation latency (Subplot 1) ######\n",
    "\n",
    "# Plot elongation latency predicted by our simulation \n",
    "ax1.scatter(phi_list,[rxndiff[d][2][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32) for d in rxndiff],zorder=2,s=250,marker='o',color='black')\n",
    "ax1.errorbar(phi_list,[rxndiff[d][2][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32) for d in rxndiff],[rxndiff[d][5][0] for d in rxndiff],zorder=1,linewidth=4,elinewidth=2,linestyle='dashed',color='black')\n",
    "ax1.set_xlabel('Voxel volume fraction $(\\phi_{vox})$',size=30,color='black')\n",
    "ax1.set_ylabel('Elongation latency (ms)',fontsize=30)\n",
    "\n",
    "#Plot experimental measurements of elongation latency from literature\n",
    "y_model,SS_err,_,_,_ = np.polyfit(phi_list,[1000/12,1000/15,1000/18,1000/19,1000/20,1000/21],2,full=True)\n",
    "y_hat = np.polyval(y_model,phi_list)\n",
    "x_sweep_i = np.linspace(min(phi_list),max(phi_list),1000)\n",
    "y_hat_sweep_i = np.polyval(y_model, x_sweep_i)\n",
    "ax1.plot(x_sweep_i,y_hat_sweep_i,marker='o',markersize=0,markeredgewidth=5,markevery=199,linewidth=4,color='black')\n",
    "ax1.fill_between(x_sweep_i, y_hat_sweep_i, 0*y_hat_sweep_i, color='green',alpha=0.3)\n",
    "\n",
    "#Output predicted and empirical values\n",
    "print('\\nElongation latencies: \\n', [rxndiff[d][2][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32) for d in rxndiff])\n",
    "print([rxndiff[d][5][0] for d in rxndiff])\n",
    "experimental_vals= [y_hat_sweep_i[0],y_hat_sweep_i[-1]]\n",
    "print('\\n\\tFor lowest and highest growth rates:')\n",
    "print('\\tExperimentally predicted values of elongation latency: ',experimental_vals)\n",
    "print('\\tPredicted/Experimental: ',np.divide([rxndiff['06'][2][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32),rxndiff['30'][2][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32)],experimental_vals))\n",
    "\n",
    "\n",
    "\n",
    "####### Plot Transport latency (Subplot 2) #####\n",
    "\n",
    "#Plot transport latency predicted by our simulation\n",
    "ax2.scatter(phi_list,[rxndiff[d][0][0] for d in rxndiff],zorder=2,s=250,marker='o',color='black')\n",
    "ax2.errorbar(phi_list,[rxndiff[d][0][0] for d in rxndiff],[rxndiff[d][3][0] for d in rxndiff],zorder=1,linewidth=4,elinewidth=2,linestyle='dashed',color='black')\n",
    "ax2.set_xlabel('Voxel volume fraction $(\\phi_{vox})$',size=30,color='black')\n",
    "ax2.set_ylabel('Transport latency (ms)',fontsize=30)\n",
    "ax2.set_yticks(np.arange(0, 226, 25))\n",
    "\n",
    "#Plot inferred experimental values of transport latency (using data from literature)\n",
    "y_model,SS_err,_,_,_ = np.polyfit(phi_list,np.subtract([1000/12,1000/16,1000/18,1000/19,1000/20,1000/21],(1000/1475+1000/1529+1000/209+1000/200+1000/32)),2,full=True)\n",
    "y_hat = np.polyval(y_model,phi_list)\n",
    "x_sweep_i = np.linspace(min(phi_list),max(phi_list),1000)\n",
    "y_hat_sweep_i = np.polyval(y_model, x_sweep_i)\n",
    "ax2.plot(x_sweep_i,y_hat_sweep_i,marker='o',markersize=0,markeredgewidth=5,markevery=199,linewidth=4,color='black',label='Empirical bulk elongation')\n",
    "ax2.fill_between(x_sweep_i, y_hat_sweep_i, 0*y_hat_sweep_i,edgecolor='black',alpha=0.3,facecolor='green')\n",
    "exptl_trans = y_hat_sweep_i\n",
    "\n",
    "#Output predicted and empirical values\n",
    "print('\\n\\nTransport latencies: \\n',[rxndiff[d][0][0] for d in rxndiff])\n",
    "print('\\n\\nTransport latency SEM: \\n', [rxndiff[d][3][0] for d in rxndiff])\n",
    "experimental_vals = [y_hat_sweep_i[0],y_hat_sweep_i[-1]]\n",
    "print('\\n\\tFor lowest and highest growth rates:')\n",
    "print('\\tExperimentally predicted values of transport latency', experimental_vals)\n",
    "print('\\tPredicted/Experimental: ',np.divide([rxndiff['06'][0][0],rxndiff['30'][0][0]],experimental_vals))\n",
    "\n",
    "\n",
    "\n",
    "### Plot Reaction latency (Subplot 3) ###\n",
    "\n",
    "#Plot reaction latency predicted by our simulation\n",
    "ax3.scatter(phi_list,[rxndiff[d][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32) for d in rxndiff],zorder=2,s=250,marker='o',color='black')\n",
    "ax3.errorbar(phi_list,[rxndiff[d][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32) for d in rxndiff],[rxndiff[d][4][0] for d in rxndiff],zorder=1,linewidth=4,elinewidth=2,linestyle='dashed',color='black')\n",
    "ax3.set_xlabel('Voxel volume fraction $(\\phi_{vox})$',size=30,color='black')\n",
    "ax3.set_ylabel('Reaction latency (ms)',fontsize=30)\n",
    "\n",
    "#Plot inferred experimental values of reaction latency (using data from literature)\n",
    "y_model,SS_err,_,_,_ = np.polyfit(phi_list,[1000/1475+1000/1529+1000/209+1000/200+1000/32]*len(phi_list),2,full=True)\n",
    "y_hat = np.polyval(y_model,phi_list)\n",
    "x_sweep_i = np.linspace(min(phi_list),max(phi_list),1000)\n",
    "y_hat_sweep_i = np.polyval(y_model, x_sweep_i)\n",
    "ax3.plot(x_sweep_i,y_hat_sweep_i,marker='o',markersize=0,markeredgewidth=5,markevery=199,linewidth=4,color='black')\n",
    "ax3.fill_between(x_sweep_i, y_hat_sweep_i, 0*y_hat_sweep_i, color='green',alpha=0.3)\n",
    "exptl_rxn = y_hat_sweep_i\n",
    "\n",
    "#Output predicted and empirical values\n",
    "print('\\n\\nReaction latencies: \\n',[rxndiff[d][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32) for d in rxndiff])\n",
    "print('\\n\\nReaction latency SEM: \\n',[rxndiff[d][4][0] for d in rxndiff])\n",
    "\n",
    "experimental_vals= [y_hat_sweep_i[0],y_hat_sweep_i[-1],y_hat_sweep_i[-1]]\n",
    "print('\\n\\tFor lowest and highest growth rates:')\n",
    "print('\\tExperimentally predicted values of reaction latency',experimental_vals)\n",
    "print('\\tPredicted/Experimental: ', np.divide([rxndiff['06'][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32),rxndiff['10'][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32),rxndiff['30'][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32)],experimental_vals))\n",
    "\n",
    "\n",
    "\n",
    "##### Plot Da (Subplot 4) ######\n",
    "\n",
    "##Plot Damkohler number predicted by our simulation\n",
    "bmTrans = np.array([rxndiff[d][0][0] for d in rxndiff])\n",
    "bmRxn = np.array([rxndiff[d][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32) for d in rxndiff])\n",
    "bmTransSEM = np.array([rxndiff[d][3][0] for d in rxndiff])\n",
    "bmRxnSEM = np.array([rxndiff[d][4][0] for d in rxndiff])\n",
    "bm_Da_var = (bmTrans**2/bmRxn**2)*(bmTransSEM**2/bmTrans**2+bmRxnSEM**2/bmRxn**2)\n",
    "print(bm_Da_var)\n",
    "ax4.scatter(phi_list, np.array([rxndiff[d][0][0] for d in rxndiff])/np.array([rxndiff[d][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32) for d in rxndiff]),zorder=2,s=250,marker='o',color='lightgray')\n",
    "ax4.errorbar(phi_list,np.array([rxndiff[d][0][0] for d in rxndiff])/np.array([rxndiff[d][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32) for d in rxndiff]),bm_Da_var,zorder=1,linewidth=4,elinewidth=2,linestyle='dashed',color='lightgray')\n",
    "ax4.set_xlabel('Voxel volume fraction $(\\phi_{vox})$',size=30,color='black')\n",
    "ax4.set_ylabel('Da',fontsize=30)\n",
    "\n",
    "## Plot pre-load Da\n",
    "preload_trans = np.array([34.01663109406401, 30.41369509254369, 29.154101488155067, 28.491437978874004, 28.409508968849106, 30.805382022214264] )\n",
    "preload_rxn =  np.array([3.537059926602166, 7.085786360511388, 7.812818601943163, 7.686728426387252, 6.542742485140954, 6.120797969362092] )\n",
    "ax4.scatter(phi_list, preload_trans/(preload_rxn+(1000/1475+1000/1529+1000/209+1000/200+1000/32)),zorder=2,s=250,marker='^',color='#0683b4')\n",
    "ax4.errorbar(phi_list, preload_trans/(preload_rxn+(1000/1475+1000/1529+1000/209+1000/200+1000/32)),0,zorder=1,linewidth=4,elinewidth=2,linestyle='dashed',color='#0683b4')\n",
    "\n",
    "### Plot pre-sort+pre-load Da\n",
    "presortpreload_trans=np.array([34.01663109406401,20.007025364980883,11.68166201748362,8.274169505141892, 7.074660404155837,  5.269097555364241])\n",
    "presortpreload_rxn = np.array([3.537059926602166,3.5651502846900125,2.0426277448724632,1.4374991110185171,1.2407775100632652,1.0742130084209447])\n",
    "ax4.scatter(phi_list, presortpreload_trans /(presortpreload_rxn+(1000/1475+1000/1529+1000/209+1000/200+1000/32)),zorder=2,s=250,marker='s',color='#bf7200')\n",
    "ax4.errorbar(phi_list, presortpreload_trans/(presortpreload_rxn+(1000/1475+1000/1529+1000/209+1000/200+1000/32)),0,zorder=1,linewidth=4,elinewidth=2,linestyle='dashed',color='#bf7200')\n",
    "\n",
    "\n",
    "#Plot inferred experimental values of Damkohler number (using data from literature)\n",
    "ax4.plot(x_sweep_i,exptl_trans/exptl_rxn,marker='o',markersize=0,markeredgewidth=5,markevery=199,linewidth=4,color='black',label='Empirical bulk elongation')\n",
    "ax4.fill_between(x_sweep_i, exptl_trans/exptl_rxn, 0*y_hat_sweep_i,edgecolor='black',alpha=0.3,facecolor='green')\n",
    "\n",
    "#Output predicted and empirical values\n",
    "print('\\n\\nDamkohler number BM-only: \\n',np.array([rxndiff[d][0][0] for d in rxndiff])/np.array([rxndiff[d][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32) for d in rxndiff]))\n",
    "experimental_vals= [exptl_trans[0]/exptl_rxn[0],exptl_trans[-1]/exptl_rxn[-1]]\n",
    "print('\\n\\tFor lowest and highest growth rates:')\n",
    "print('\\tExperimentally predicted values of Da',experimental_vals)\n",
    "print('\\tPredicted/Experimental: ',np.divide([rxndiff['06'][0][0]/(rxndiff['06'][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32)),rxndiff['30'][0][0]/(rxndiff['30'][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32))],experimental_vals))\n",
    "\n",
    "#Output predicted and empirical values\n",
    "print('\\n\\nDamkohler number preload: \\n',preload_trans/(preload_rxn+(1000/1475+1000/1529+1000/209+1000/200+1000/32)))\n",
    "experimental_vals= [exptl_trans[0]/exptl_rxn[0],exptl_trans[-1]/exptl_rxn[-1]]\n",
    "print('\\n\\tFor lowest and highest growth rates:')\n",
    "print('\\tExperimentally predicted values of Da',experimental_vals)\n",
    "print('\\tPredicted/Experimental: ',np.divide(np.array( [34.01663109406401, 30.805382022214264] \n",
    ")/(np.array([3.537059926602166,  6.120797969362092] \n",
    ")+(1000/1475+1000/1529+1000/209+1000/200+1000/32)),experimental_vals))\n",
    "\n",
    "#Output predicted and empirical values\n",
    "print('\\n\\nDamkohler number preload-presort: \\n',presortpreload_trans/(presortpreload_rxn+(1000/1475+1000/1529+1000/209+1000/200+1000/32)))\n",
    "experimental_vals= [exptl_trans[0]/exptl_rxn[0],exptl_trans[-1]/exptl_rxn[-1]]\n",
    "print('\\n\\tFor lowest and highest growth rates:')\n",
    "print('\\tExperimentally predicted values of Da',experimental_vals)\n",
    "print('\\tPredicted/Experimental: ',np.divide([presorttrans/(presortrxn+(1000/1475+1000/1529+1000/209+1000/200+1000/32))][0],experimental_vals[0]))\n",
    "\n",
    "\n",
    "####Plot building/Axis labeling####\n",
    "for _,axes in enumerate([ax1,ax2,ax3,ax4]):\n",
    "    axes.spines['left'].set_linewidth(4)\n",
    "    axes.spines['bottom'].set_linewidth(4)\n",
    "    axes.spines['top'].set_linewidth(4)\n",
    "    axes.spines['right'].set_linewidth(4)\n",
    "    axes.tick_params(axis='both', which='major', labelsize=20)\n",
    "    axes.set_xlim(0.11,0.44)\n",
    "    \n",
    "    ax1 = axes.twiny()\n",
    "    ax1.set_xlim(0.11,0.44)\n",
    "    ax1.tick_params(axis='both', which='major',width=4, labelsize=20)\n",
    "    ax1.set_xticks(phi_list)\n",
    "    ax1.set_xlabel('Growth rate (dbl/hr)',size=30,color='black',labelpad=20)\n",
    "    labels = [item.get_text() for item in ax1.get_xticklabels()]\n",
    "    labels = ['0.6','1.0','1.5','2.0','2.5','3.0']\n",
    "    ax1.set_xticklabels(labels)\n",
    "\n",
    "\n",
    "plt.tight_layout(pad=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_std = np.array([rxndiff[d][3][0] for d in rxndiff])\n",
    "rxn_std = np.array([rxndiff[d][4][0] for d in rxndiff])\n",
    "trans_mean = np.array([rxndiff[d][0][0] for d in rxndiff])\n",
    "rxn_mean = np.array([rxndiff[d][1][0]+(1000/1529+1000/209+1000/200+1000/32) for d in rxndiff])\n",
    "(trans_std**2)/(rxn_std)**2-(trans_mean*rxn_mean)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Kinetic Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "path = \"/Users/Akshay/Documents/TranslationDynamics/data/\"\n",
    "\n",
    "gr_sim = dict()\n",
    "rxndiff_1=dict()\n",
    "\n",
    "data = \"200511_1738/\"\n",
    "gr_sim = dict()\n",
    "\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=3000)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "rxndiff_1['3000'] = transportRxnCalc(gr_dict,gr25_ptRNA,gr25_pCodon)[1:]\n",
    "\n",
    "data = \"200511_2104/\"\n",
    "gr_sim = dict()\n",
    "\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=2000)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "rxndiff_1['2000'] = transportRxnCalc(gr_dict,gr25_ptRNA,gr25_pCodon)[1:]\n",
    "\n",
    "data = \"200511_2155/\"\n",
    "gr_sim = dict()\n",
    "\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=1000)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "rxndiff_1['1000'] = transportRxnCalc(gr_dict,gr25_ptRNA,gr25_pCodon)[1:]\n",
    "\n",
    "data = \"200511_2229/\"\n",
    "gr_sim = dict()\n",
    "\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=800)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "rxndiff_1['800'] = transportRxnCalc(gr_dict,gr25_ptRNA,gr25_pCodon)[1:]\n",
    "\n",
    "\n",
    "data = \"200511_2302/\"\n",
    "gr_sim = dict()\n",
    "\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=600)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "rxndiff_1['600'] = transportRxnCalc(gr_dict,gr25_ptRNA,gr25_pCodon)[1:]\n",
    "\n",
    "\n",
    "data = \"200512_0110/\"\n",
    "gr_sim = dict()\n",
    "\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=400)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "rxndiff_1['400'] = transportRxnCalc(gr_dict,gr25_ptRNA,gr25_pCodon)[1:]\n",
    "\n",
    "\n",
    "data = \"191003_2210/\"\n",
    "gr_sim = dict()\n",
    "\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=200)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "rxndiff_1['200'] = transportRxnCalc(gr_dict,gr25_ptRNA,gr25_pCodon)[1:]\n",
    "\n",
    "data = \"200512_1452/\"\n",
    "gr_sim = dict()\n",
    "\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=100)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "rxndiff_1['100'] = transportRxnCalc(gr_dict,gr25_ptRNA,gr25_pCodon)[1:]\n",
    "\n",
    "data = \"200512_1716/\"\n",
    "gr_sim = dict()\n",
    "\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=50)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "rxndiff_1['50'] = transportRxnCalc(gr_dict,gr25_ptRNA,gr25_pCodon)[1:]\n",
    "\n",
    "data = \"200512_2150/\"\n",
    "gr_sim = dict()\n",
    "\n",
    "for i in range(1,7):\n",
    "    sim_i = computeTransportRxnTimes(path+data, 1e15, 1e15, 100*(i-1),100*(i-1)+100,scaling=25)\n",
    "    gr_sim[i] = (CellLatencies(sim_i))\n",
    "\n",
    "gr_dict = {'gr_1':gr_sim}\n",
    "rxndiff_1['25'] = transportRxnCalc(gr_dict,gr25_ptRNA,gr25_pCodon)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, [ax1,ax2,ax3,ax4] = plt.subplots(1,4,figsize=(27,6))\n",
    "x=[3000,2000,1000,800,600,400,200,100,50,25]\n",
    "\n",
    "ax1.scatter(x,[rxndiff_1[d][2][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32) for d in rxndiff_1],zorder=2,s=250,color='black')\n",
    "ax1.errorbar(x,[rxndiff_1[d][2][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32) for d in rxndiff_1],[rxndiff_1[d][5][0] for d in rxndiff_1],zorder=1,linewidth=0,elinewidth=2,linestyle='dashed',color='black')\n",
    "y_model= np.polyfit(x,[rxndiff_1[d][2][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32) for d in rxndiff_1],3)\n",
    "trendpoly=np.poly1d(y_model)\n",
    "ax1.plot(np.linspace(3000, 0, 100),trendpoly(np.linspace(3000, 0, 100)),zorder=1,linewidth=4,linestyle='dashed',color='black')\n",
    "ax1.set_xlabel('Kinetic rate scaling',size=25,color='black')\n",
    "ax1.set_ylabel('Elongation latency (ms)',size=25,color='black')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xticks((10,100,1000,3000))\n",
    "\n",
    "\n",
    "ax2.scatter(x,[rxndiff_1[d][0][0] for d in rxndiff_1],zorder=2,s=250,color='black')\n",
    "ax2.errorbar(x,[rxndiff_1[d][0][0] for d in rxndiff_1],[rxndiff_1[d][3][0] for d in rxndiff_1],zorder=1,linewidth=0,elinewidth=2,linestyle='dashed',color='black')\n",
    "y_model= np.polyfit(x,[rxndiff_1[d][0][0] for d in rxndiff_1],3)\n",
    "trendpoly=np.poly1d(y_model)\n",
    "ax2.plot(np.linspace(3000, 0, 100),trendpoly(np.linspace(3000, 0, 100)),zorder=1,linewidth=4,linestyle='dashed',color='black')\n",
    "ax2.set_xlabel('Kinetic rate scaling',size=25,color='black')\n",
    "ax2.set_ylabel('Transport latency (ms)',size=25,color='black')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xticks((10,100,1000,3000))\n",
    "\n",
    "\n",
    "ax3.scatter(x,[rxndiff_1[d][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32)  for d in rxndiff_1],zorder=2,s=250,color='black')\n",
    "ax3.errorbar(x,[rxndiff_1[d][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32)  for d in rxndiff_1],[rxndiff_1[d][4][0] for d in rxndiff_1],zorder=1,linewidth=0,elinewidth=2,linestyle='dashed',color='black')\n",
    "y_model= np.polyfit(x,[rxndiff_1[d][1][0]+(1000/1475+1000/1529+1000/209+1000/200+1000/32)  for d in rxndiff_1],2)\n",
    "trendpoly=np.poly1d(y_model)\n",
    "ax3.plot(np.linspace(3000, 0, 100),trendpoly(np.linspace(3000, 0, 100)),zorder=1,linewidth=4,linestyle='dashed',color='black')\n",
    "ax3.set_xlabel('Kinetic rate scaling',size=25,color='black')\n",
    "ax3.set_ylabel('Reaction latency (ms)',size=25,color='black')\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_xticks((10,100,1000,3000))\n",
    "\n",
    "#######################\n",
    "voxlen = 0.0677\n",
    "D = 56*0.284\n",
    "mixTime = []\n",
    "t1 = 1/717\n",
    "scalingTime = []\n",
    "scaling = np.arange(0,3000,1)\n",
    "\n",
    "for i in scaling:\n",
    "    scalingTime.append(t1/i*1e3)\n",
    "    mixTime.append((0.012)**2/(6*D)*1e3)\n",
    "    \n",
    "ax4.plot(scaling,mixTime,'grey',linewidth=4,linestyle='dashed',color='black')\n",
    "ax4.plot(scaling,scalingTime,'black',linewidth=4)\n",
    "ax4.set_yscale('log')\n",
    "ax4.set_xlim(0,3000)\n",
    "ax4.set_xlabel('Kinetic rate scaling',size=25)\n",
    "ax4.set_ylabel('Time (ms)',size=25)\n",
    "\n",
    "print(scalingTime[925:950])\n",
    "#######################\n",
    "\n",
    "for _,axes in enumerate([ax1,ax2,ax3,ax4]):\n",
    "    axes.tick_params(axis='both', which='major',width=4, labelsize=30)\n",
    "    axes.spines['left'].set_linewidth(4)\n",
    "    axes.spines['bottom'].set_linewidth(4)\n",
    "    axes.spines['top'].set_linewidth(4)\n",
    "    axes.spines['right'].set_linewidth(4)\n",
    "    axes.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "plt.tight_layout(pad=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot MSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from scipy.stats import linregress\n",
    "\n",
    "%matplotlib inline\n",
    "def MSD(path,expt_start,expt_end,moltype=''):\n",
    "    df_outputs = pd.read_csv(path+\"outputMolPosList.txt\",sep=\" \",header=None) #Add batch processing here potentially\n",
    "    if moltype==\"ribosome\":\n",
    "         df_outputs = df_outputs.iloc[:,0].str.replace('MolpostRNA' , 'MolposRibosome')\n",
    "    if moltype==\"tRNA\":\n",
    "        df_outputs = df_outputs.iloc[:,0].str.replace('MolpostRNA' , 'MolpostRNA')\n",
    "    if moltype==\"crowder\":\n",
    "        df_outputs = df_outputs.iloc[:,0].str.replace('MolpostRNA' , 'MolposCrowder')\n",
    "    r2_lists=list()\n",
    "    for expt_num, row in enumerate(df_outputs):\n",
    "        if(expt_num>=expt_start and expt_num<expt_end):\n",
    "            try:\n",
    "                df = pd.read_csv(path+row,delimiter=\" \",header=None)\n",
    "                params = pd.read_csv(path+\"expt_\"+str(expt_num)+'.txt',delim_whitespace=True,names=['var','param','val'])\n",
    "                molposTS = float(params.loc[params['param']=='Z_molPosTS_']['val'].values[0])\n",
    "                df.columns=[\"time\",\"r2\",\"r4\"]\n",
    "                r2_lists.append(np.array(df['r2'].tolist()))\n",
    "            except:\n",
    "                print(\"Error (e.g., Missing expt or incorrect input)\")\n",
    "    return np.array(r2_lists)*0.0059**2,np.add.reduce(r2_lists)/(len(r2_lists))*0.0059**2,np.arange(0,len(np.add.reduce(r2_lists)/(len(r2_lists))))*molposTS*6.21607E-07\n",
    "\n",
    "def MSD_noise(MSD_vals):\n",
    "    MSD_list = list()\n",
    "    for sim in MSD_vals[0]:\n",
    "        MSD_list.append(linregress(MSD_vals[2][0:-1],sim[0:-1])[0]/2)\n",
    "    #N = 1000\n",
    "    #bootstrap_MSD=list()\n",
    "    #for i in range(N):\n",
    "    #    bootstrap_MSD.append(np.average((np.random.choice(MSD_list,len(MSD_list)))))        \n",
    "    return np.std(MSD_list/np.sqrt(len(MSD_list)))\n",
    "#crowder,ribosome,tRNA\n",
    "path='/Users/Akshay/Documents/TranslationDynamics/data/'\n",
    "data = \"190927_0408/\" \n",
    "n = 10\n",
    "slopes = list()\n",
    "slope_sems = list()\n",
    "res_steps=3\n",
    "k=0\n",
    "start=0\n",
    "end=-1\n",
    "species = 'tRNA'\n",
    "\n",
    "plt.rc('mathtext', fontset=\"cm\")\n",
    "fig,ax=plt.subplots(1,1,figsize=(7,6))\n",
    "colors = ['black','green','purple','blue','red','brown','orange']\n",
    "\n",
    "MSD_vals=MSD(path+'190930_1418/',0,20,species)\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='black',label=\"Dilute limit\",linewidth=3)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k,k+n,species)\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='green',label=\"gr=0.6\",linewidth=3)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals = MSD(path+data,k+n,k+2*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps*1),MSD_vals[1][start:end],color='purple',label=\"gr=1.0\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k+2*n,k+3*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='blue',label=\"gr=1.5\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k+3*n,k+4*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='red',label=\"gr=2.0\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k+4*n,k+5*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='brown',label=\"gr=2.5\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k+5*n,k+6*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='orange',label=\"gr=3.0\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "leg = ax.legend(fontsize=15)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Time (s)\",size=30,labelpad=10)\n",
    "ax.set_ylabel(r\"MSD ($\\mu^2$)\",size=30,labelpad=10)\n",
    "\n",
    "diff_list = list()\n",
    "gr_list = [0,0.6,1.0,1.5,2.0,2.5,3.0]\n",
    "phi_list = [0,0.132,0.224,0.302,0.356,0.395,0.423]\n",
    "\n",
    "for i in range(len(slopes)):\n",
    "    plt.text(1.02,0.93,r\"Diffusivity ($\\mu^2/s$)\",color='black',transform=plt.gca().transAxes,fontsize=20)\n",
    "    plt.text(1.04,0.85-0.07*i,str(np.round(np.array(slopes)*res_steps/6,1)[i])+' +/- '+str(np.round(slope_sems[i],1)),color=colors[i],transform=plt.gca().transAxes,fontsize=20)\n",
    "    diff_list.append(np.round(np.array(slopes)*res_steps/6,3)[i])\n",
    "print(diff_list)\n",
    "\n",
    "\n",
    "ax.spines['left'].set_linewidth(4)\n",
    "ax.spines['bottom'].set_linewidth(4)\n",
    "ax.spines['top'].set_linewidth(4)\n",
    "ax.spines['right'].set_linewidth(4)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax.set_xlim(0,500*time)\n",
    "ax.set_ylim(0,0.10)\n",
    "ax.set_title('Smoldyn',size=30,pad=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/Akshay/Documents/TranslationDynamics/data/'\n",
    "data = \"20200511_LAMMPS_MSD/\" \n",
    "slopes = list()\n",
    "\n",
    "plt.rc('mathtext', fontset=\"cm\")\n",
    "fig,ax=plt.subplots(1,1,figsize=(7,6))\n",
    "colors = ['green','purple','blue','red','brown','orange']\n",
    "\n",
    "length = 0.0059**2\n",
    "time = 6.21607E-07\n",
    "\n",
    "\n",
    "df_outputs = pd.read_csv(path+data+\"MSD_gr_06_.csv\",header=None)\n",
    "ax.plot(df_outputs[0]*time,df_outputs[2]*length,color='green',label=\"gr=0.6\",linewidth=3)\n",
    "slopes.append(np.linalg.lstsq((df_outputs[0]*time)[:,np.newaxis],df_outputs[2]*length,rcond=None)[0][0])\n",
    "\n",
    "df_outputs = pd.read_csv(path+data+\"MSD_gr_10_.csv\",header=None)\n",
    "ax.plot(df_outputs[0]*time,df_outputs[2]*length,color='purple',label=\"gr=1.0\",linewidth=3)\n",
    "slopes.append(np.linalg.lstsq((df_outputs[0]*time)[:,np.newaxis],df_outputs[2]*length,rcond=None)[0][0])\n",
    "\n",
    "df_outputs = pd.read_csv(path+data+\"MSD_gr_15_.csv\",header=None)\n",
    "ax.plot(df_outputs[0]*time,df_outputs[2]*length,color='blue',label=\"gr=1.5\",linewidth=3)\n",
    "slopes.append(np.linalg.lstsq((df_outputs[0]*time)[:,np.newaxis],df_outputs[2]*length,rcond=None)[0][0])\n",
    "\n",
    "df_outputs = pd.read_csv(path+data+\"MSD_gr_20_.csv\",header=None)\n",
    "ax.plot(df_outputs[0]*time,df_outputs[2]*length,color='red',label=\"gr=2.0\",linewidth=3)\n",
    "slopes.append(np.linalg.lstsq((df_outputs[0]*time)[:,np.newaxis],df_outputs[2]*length,rcond=None)[0][0])\n",
    "\n",
    "df_outputs = pd.read_csv(path+data+\"MSD_gr_25_.csv\",header=None)\n",
    "ax.plot(df_outputs[0]*time,df_outputs[2]*length,color='brown',label=\"gr=2.5\",linewidth=3)\n",
    "slopes.append(np.linalg.lstsq((df_outputs[0]*time)[:,np.newaxis],df_outputs[2]*length,rcond=None)[0][0])\n",
    "\n",
    "df_outputs = pd.read_csv(path+data+\"MSD_gr_30_.csv\",header=None)\n",
    "ax.plot(df_outputs[0]*time,df_outputs[2]*length,color='orange',label=\"gr=3.0\",linewidth=3)\n",
    "slopes.append(np.linalg.lstsq((df_outputs[0]*time)[:,np.newaxis],df_outputs[2]*length,rcond=None)[0][0])\n",
    "\n",
    "\n",
    "leg = ax.legend(fontsize=15)\n",
    "\n",
    "ax.set_xlabel(\"Time (s)\",size=30,labelpad=10)\n",
    "ax.set_ylabel(r\"MSD ($\\mu^2$)\",size=30,labelpad=10)\n",
    "\n",
    "diff_list = list()\n",
    "gr_list = [0,0.6,1.0,1.5,2.0,2.5,3.0]\n",
    "phi_list = [0,0.132,0.224,0.302,0.356,0.395,0.423]\n",
    "\n",
    "for i in range(len(slopes)):\n",
    "    plt.text(1.02,0.93,r\"Diffusivity ($\\mu^2/s$)\",color='black',transform=plt.gca().transAxes,fontsize=20)\n",
    "    plt.text(1.08,0.85-0.07*i,str(np.round(np.array(slopes)/6,1)[i])+'+/- 0.0',color=colors[i],transform=plt.gca().transAxes,fontsize=20)\n",
    "    diff_list.append(np.round(np.array(slopes)/6,3)[i])\n",
    "print(diff_list)\n",
    "\n",
    "\n",
    "ax.spines['left'].set_linewidth(4)\n",
    "ax.spines['bottom'].set_linewidth(4)\n",
    "ax.spines['top'].set_linewidth(4)\n",
    "ax.spines['right'].set_linewidth(4)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax.set_xlim(0,500*time)\n",
    "ax.set_ylim(0,0.10)\n",
    "ax.set_title('LAMMPS',size=30,pad=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from scipy.stats import linregress\n",
    "\n",
    "%matplotlib inline\n",
    "def MSD(path,expt_start,expt_end,moltype=''):\n",
    "    df_outputs = pd.read_csv(path+\"outputMolPosList.txt\",sep=\" \",header=None) #Add batch processing here potentially\n",
    "    if moltype==\"ribosome\":\n",
    "         df_outputs = df_outputs.iloc[:,0].str.replace('MolpostRNA' , 'MolposRibosome')\n",
    "    if moltype==\"tRNA\":\n",
    "        df_outputs = df_outputs.iloc[:,0].str.replace('MolpostRNA' , 'MolpostRNA')\n",
    "    if moltype==\"crowder\":\n",
    "        df_outputs = df_outputs.iloc[:,0].str.replace('MolpostRNA' , 'MolposCrowder')\n",
    "    r2_lists=list()\n",
    "    for expt_num, row in enumerate(df_outputs):\n",
    "        if(expt_num>=expt_start and expt_num<expt_end):\n",
    "            try:\n",
    "                df = pd.read_csv(path+row,delimiter=\" \",header=None)\n",
    "                params = pd.read_csv(path+\"expt_\"+str(expt_num)+'.txt',delim_whitespace=True,names=['var','param','val'])\n",
    "                molposTS = float(params.loc[params['param']=='Z_molPosTS_']['val'].values[0])\n",
    "                df.columns=[\"time\",\"r2\",\"r4\"]\n",
    "                r2_lists.append(np.array(df['r2'].tolist()))\n",
    "            except:\n",
    "                print(\"Error (e.g., Missing expt or incorrect input)\")\n",
    "    return np.array(r2_lists)*0.0059**2,np.add.reduce(r2_lists)/(len(r2_lists))*0.0059**2,np.arange(0,len(np.add.reduce(r2_lists)/(len(r2_lists))))*molposTS*6.21607E-07\n",
    "\n",
    "def MSD_noise(MSD_vals):\n",
    "    MSD_list = list()\n",
    "    for sim in MSD_vals[0]:\n",
    "        MSD_list.append(linregress(MSD_vals[2][0:-1],sim[0:-1])[0]/2)\n",
    "    #N = 1000\n",
    "    #bootstrap_MSD=list()\n",
    "    #for i in range(N):\n",
    "    #    bootstrap_MSD.append(np.average((np.random.choice(MSD_list,len(MSD_list)))))        \n",
    "    return np.std(MSD_list/np.sqrt(len(MSD_list)))\n",
    "#crowder,ribosome,tRNA\n",
    "path='/Users/Akshay/Documents/TranslationDynamics/data/'\n",
    "data = \"200523_2242/\" \n",
    "n = 10\n",
    "slopes = list()\n",
    "slope_sems = list()\n",
    "res_steps=3\n",
    "k=0\n",
    "start=0\n",
    "end=-1\n",
    "species = 'tRNA'\n",
    "\n",
    "plt.rc('mathtext', fontset=\"cm\")\n",
    "fig,ax=plt.subplots(1,1,figsize=(7,6))\n",
    "colors = ['black','green','purple','blue','red','brown','orange']\n",
    "\n",
    "MSD_vals=MSD(path+'190930_1418/',0,20,species)\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='black',label=\"Dilute limit\",linewidth=3)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k,k+n,species)\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='green',label=\"gr=0.6\",linewidth=3)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals = MSD(path+data,k+n,k+2*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps*1),MSD_vals[1][start:end],color='purple',label=\"gr=1.0\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k+2*n,k+3*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='blue',label=\"gr=1.5\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k+3*n,k+4*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='red',label=\"gr=2.0\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k+4*n,k+5*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='brown',label=\"gr=2.5\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k+5*n,k+6*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='orange',label=\"gr=3.0\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "leg = ax.legend(fontsize=15)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Time (s)\",size=30,labelpad=10)\n",
    "ax.set_ylabel(r\"MSD ($\\mu^2$)\",size=30,labelpad=10)\n",
    "\n",
    "diff_list = list()\n",
    "gr_list = [0,0.6,1.0,1.5,2.0,2.5,3.0]\n",
    "phi_list = [0,0.132,0.224,0.302,0.356,0.395,0.423]\n",
    "\n",
    "for i in range(len(slopes)):\n",
    "    plt.text(1.02,0.93,r\"Diffusivity ($\\mu^2/s$)\",color='black',transform=plt.gca().transAxes,fontsize=20)\n",
    "    plt.text(1.04,0.85-0.07*i,str(np.round(np.array(slopes)*res_steps/6,1)[i])+' +/- '+str(np.round(slope_sems[i],1)),color=colors[i],transform=plt.gca().transAxes,fontsize=20)\n",
    "    diff_list.append(np.round(np.array(slopes)*res_steps/6,3)[i])\n",
    "print(diff_list)\n",
    "\n",
    "\n",
    "ax.spines['left'].set_linewidth(4)\n",
    "ax.spines['bottom'].set_linewidth(4)\n",
    "ax.spines['top'].set_linewidth(4)\n",
    "ax.spines['right'].set_linewidth(4)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax.set_xlim(0,500*time)\n",
    "ax.set_ylim(0,0.10)\n",
    "ax.set_title('Smoldyn (no crowders)',size=30,pad=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from scipy.stats import linregress\n",
    "\n",
    "%matplotlib inline\n",
    "def MSD(path,expt_start,expt_end,moltype=''):\n",
    "    df_outputs = pd.read_csv(path+\"outputMolPosList.txt\",sep=\" \",header=None) #Add batch processing here potentially\n",
    "    if moltype==\"ribosome\":\n",
    "         df_outputs = df_outputs.iloc[:,0].str.replace('MolpostRNA' , 'MolposRibosome')\n",
    "    if moltype==\"tRNA\":\n",
    "        df_outputs = df_outputs.iloc[:,0].str.replace('MolpostRNA' , 'MolpostRNA')\n",
    "    if moltype==\"crowder\":\n",
    "        df_outputs = df_outputs.iloc[:,0].str.replace('MolpostRNA' , 'MolposCrowder')\n",
    "    r2_lists=list()\n",
    "    for expt_num, row in enumerate(df_outputs):\n",
    "        if(expt_num>=expt_start and expt_num<expt_end):\n",
    "            try:\n",
    "                df = pd.read_csv(path+row,delimiter=\" \",header=None)\n",
    "                params = pd.read_csv(path+\"expt_\"+str(expt_num)+'.txt',delim_whitespace=True,names=['var','param','val'])\n",
    "                molposTS = float(params.loc[params['param']=='Z_molPosTS_']['val'].values[0])\n",
    "                df.columns=[\"time\",\"r2\",\"r4\"]\n",
    "                r2_lists.append(np.array(df['r2'].tolist()))\n",
    "            except:\n",
    "                print(\"Error (e.g., Missing expt or incorrect input)\")\n",
    "    return np.array(r2_lists)*0.0059**2,np.add.reduce(r2_lists)/(len(r2_lists))*0.0059**2,np.arange(0,len(np.add.reduce(r2_lists)/(len(r2_lists))))*molposTS*6.21607E-07\n",
    "\n",
    "def MSD_noise(MSD_vals):\n",
    "    MSD_list = list()\n",
    "    for sim in MSD_vals[0]:\n",
    "        MSD_list.append(linregress(MSD_vals[2][0:-1],sim[0:-1])[0]/2)\n",
    "    #N = 1000\n",
    "    #bootstrap_MSD=list()\n",
    "    #for i in range(N):\n",
    "    #    bootstrap_MSD.append(np.average((np.random.choice(MSD_list,len(MSD_list)))))        \n",
    "    return np.std(MSD_list/np.sqrt(len(MSD_list)))\n",
    "#crowder,ribosome,tRNA\n",
    "path='/Users/Akshay/Documents/TranslationDynamics/data/'\n",
    "data = \"200524_1247/\" \n",
    "n = 10\n",
    "slopes = list()\n",
    "slope_sems = list()\n",
    "res_steps=1\n",
    "k=0\n",
    "start=0\n",
    "end=-1\n",
    "species = 'tRNA'\n",
    "\n",
    "plt.rc('mathtext', fontset=\"cm\")\n",
    "fig,ax=plt.subplots(1,1,figsize=(7,6))\n",
    "colors = ['green','purple','blue','red','brown','orange']\n",
    "\n",
    "#MSD_vals=MSD(path+'190930_1418/',0,20,species)\n",
    "#ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='black',label=\"Dilute limit\",linewidth=3)\n",
    "#slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "#slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k,k+n,species)\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='green',label=\"gr=0.6\",linewidth=3)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals = MSD(path+data,k+n,k+2*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps*1),MSD_vals[1][start:end],color='purple',label=\"gr=1.0\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k+2*n,k+3*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='blue',label=\"gr=1.5\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k+3*n,k+4*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='red',label=\"gr=2.0\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k+4*n,k+5*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='brown',label=\"gr=2.5\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "MSD_vals=MSD(path+data,k+5*n,k+6*n,species)\n",
    "slopes.append(np.linalg.lstsq(MSD_vals[2][start:end][:,np.newaxis],MSD_vals[1][start:end],rcond=None)[0][0])\n",
    "ax.plot(MSD_vals[2][start:end]/(res_steps),MSD_vals[1][start:end],color='orange',label=\"gr=3.0\",linewidth=3)\n",
    "slope_sems.append(MSD_noise(MSD_vals))\n",
    "\n",
    "leg = ax.legend(fontsize=15)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Time (s)\",size=30,labelpad=10)\n",
    "ax.set_ylabel(r\"MSD ($\\mu^2$)\",size=30,labelpad=10)\n",
    "\n",
    "diff_list = list()\n",
    "gr_list = [0,0.6,1.0,1.5,2.0,2.5,3.0]\n",
    "phi_list = [0,0.132,0.224,0.302,0.356,0.395,0.423]\n",
    "\n",
    "for i in range(len(slopes)):\n",
    "    plt.text(1.02,0.93,r\"Diffusivity ($\\mu^2/s$)\",color='black',transform=plt.gca().transAxes,fontsize=20)\n",
    "    plt.text(1.04,0.85-0.07*i,str(np.round(np.array(slopes)*res_steps/6,1)[i])+' +/- '+str(np.round(slope_sems[i],1)),color=colors[i],transform=plt.gca().transAxes,fontsize=20)\n",
    "    diff_list.append(np.round(np.array(slopes)*res_steps/6,3)[i])\n",
    "print(diff_list)\n",
    "\n",
    "\n",
    "ax.spines['left'].set_linewidth(4)\n",
    "ax.spines['bottom'].set_linewidth(4)\n",
    "ax.spines['top'].set_linewidth(4)\n",
    "ax.spines['right'].set_linewidth(4)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax.set_xlim(0,500*time)\n",
    "ax.set_ylim(0,0.10)\n",
    "ax.set_title('Smoldyn (No overlap resolution)',size=30,pad=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
